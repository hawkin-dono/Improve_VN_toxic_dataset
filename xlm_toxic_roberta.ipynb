{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# %pip install transformers\n","# %pip install --upgrade --force-reinstall scipy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:19:59.437086Z","iopub.status.busy":"2024-01-17T08:19:59.436259Z","iopub.status.idle":"2024-01-17T08:20:05.999551Z","shell.execute_reply":"2024-01-17T08:20:05.998761Z","shell.execute_reply.started":"2024-01-17T08:19:59.437053Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]}],"source":["\n","import transformers\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ast\n","import os\n","import re\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","\n","from sklearn import metrics\n","from tqdm import tqdm\n","from torch.utils.data import random_split\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:06.001574Z","iopub.status.busy":"2024-01-17T08:20:06.001027Z","iopub.status.idle":"2024-01-17T08:20:06.053948Z","shell.execute_reply":"2024-01-17T08:20:06.052942Z","shell.execute_reply.started":"2024-01-17T08:20:06.001544Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Available GPUs:\n","GPU 0: Tesla V100-SXM2-16GB\n","GPU 1: Tesla V100-SXM2-16GB\n","GPU 2: Tesla V100-SXM2-16GB\n","GPU 3: Tesla V100-SXM2-16GB\n"]}],"source":["import torch\n","\n","num_gpus = torch.cuda.device_count()\n","gpu_list = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n","\n","print(\"Available GPUs:\")\n","for i, gpu in enumerate(gpu_list):\n","    print(f\"GPU {i}: {gpu}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["device = 'cuda:0'"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["args = {\n","    \"model_name\": \"xlm_roberta\",\n","    \"max_length\": 300,\n","    \"batch_size\": 8,\n","    \"tranfer_learning_rate\": 1e-3,\n","    \"transfer_epochs\": 1,\n","    \"finetune_learning_rate\": 1e-5,\n","    \"fine_tune_epochs\": 10,\n","    \"device\": device,\n","    \"seed\": 42,\n","    \"test_size\": 0.2,\n","    \"train_size\": 0.8,\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhieuvut123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hpc/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# import the library\n","import wandb\n","wandb.login(key = 'a8d3f2f19fe58ead28cafb05be2d1cc8931944b3')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["Finishing last run (ID:04m4g68y) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">different-bush-5</strong> at: <a href='https://wandb.ai/hieuvut123/xlm_roberta_iter1.2/runs/04m4g68y' target=\"_blank\">https://wandb.ai/hieuvut123/xlm_roberta_iter1.2/runs/04m4g68y</a><br/> View project at: <a href='https://wandb.ai/hieuvut123/xlm_roberta_iter1.2' target=\"_blank\">https://wandb.ai/hieuvut123/xlm_roberta_iter1.2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240523_235537-04m4g68y/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:04m4g68y). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/hpc/gamma/22022515_VuTrungHieu/Toxic_dataset_improving/wandb/run-20240523_235549-31vt4k09</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hieuvut123/xlm_roberta_origin_data/runs/31vt4k09' target=\"_blank\">volcanic-snow-1</a></strong> to <a href='https://wandb.ai/hieuvut123/xlm_roberta_origin_data' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hieuvut123/xlm_roberta_origin_data' target=\"_blank\">https://wandb.ai/hieuvut123/xlm_roberta_origin_data</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hieuvut123/xlm_roberta_origin_data/runs/31vt4k09' target=\"_blank\">https://wandb.ai/hieuvut123/xlm_roberta_origin_data/runs/31vt4k09</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# start a new experiment\n","wandb.init(project=\"xlm_roberta_origin_data\")\n","#‚ÄÉcapture a dictionary of hyperparameters with config\n","wandb.config = {\"learning_rate\": args['finetune_learning_rate'], \"epochs\": args['fine_tune_epochs'], \"batch_size\": args['batch_size']}\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","# **Importing and Pre-Processing the domain data**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:06.055726Z","iopub.status.busy":"2024-01-17T08:20:06.055316Z","iopub.status.idle":"2024-01-17T08:20:06.063109Z","shell.execute_reply":"2024-01-17T08:20:06.062028Z","shell.execute_reply.started":"2024-01-17T08:20:06.055680Z"},"trusted":true},"outputs":[],"source":["train_path = 'origin/train.csv'\n","val_path = 'origin/val.csv'\n","test_path = 'origin/test.csv'"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:08.898328Z","iopub.status.busy":"2024-01-17T08:20:08.897950Z","iopub.status.idle":"2024-01-17T08:20:10.401674Z","shell.execute_reply":"2024-01-17T08:20:10.400655Z","shell.execute_reply.started":"2024-01-17T08:20:08.898291Z"},"trusted":true},"outputs":[{"ename":"ParserError","evalue":"Error tokenizing data. C error: EOF inside string starting at row 6344","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_path)\n\u001b[1;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(val_path)    \n\u001b[0;32m----> 3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n","File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 6344"]}],"source":["train_data = pd.read_csv(train_path)\n","val_data = pd.read_csv(val_path)    \n","test_data = pd.read_csv(test_path)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_data['text'] = train_data['text'].apply(str)\n","val_data['text'] = val_data['text'].apply(str)\n","test_data['text'] = test_data['text'].apply(str)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>dataset_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Ch·∫Øc c≈©ng bi·∫øt ko t·ªìn t·∫°i ƒëc bao l√¢u n·ªØa n√™n c...</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9</td>\n","      <td>Emma Nguyen v·∫≠y m√† ca ng·ª£i b√™n kia l·∫Øm... V·ªÅ l...</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11</td>\n","      <td>Nhung Pham Ai ch√™ ngh√®o ngta c√≥ ti·ªÅn ƒëi n∆∞·ªõc n...</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13</td>\n","      <td>·ª¶a b√™ ƒë√™ c√≥ lol h·∫£ ta ??üòÇüòÇüòÇ</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>16</td>\n","      <td>L√†m th√™m ch·ª•c c·ªß m·ªü h√≤m csgo anh √™i :V</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22999</th>\n","      <td>46002</td>\n","      <td>trong tr∆∞·ªùng h·ª£p n√†y, theo ls. khanh, b√† h·∫±ng...</td>\n","      <td>1</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>23000</th>\n","      <td>46003</td>\n","      <td>b√† c≈©ng kh√¥ng √≠t l·∫ßn ch·ªâ tr√≠ch n·∫∑ng n·ªÅ nh·ªØng ...</td>\n","      <td>0</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>23001</th>\n","      <td>46004</td>\n","      <td>‚Äúb√† nguy·ªÖn ph∆∞∆°ng h·∫±ng c√≥ nhi·ªÅu bi·ªÉu hi·ªán ƒë√£ ...</td>\n","      <td>0</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>23002</th>\n","      <td>46005</td>\n","      <td>v≈© ƒë·ª©c khanh n√≥i. b√† nguy·ªÖn ph∆∞∆°ng h·∫±ng b·∫Øt ƒë...</td>\n","      <td>0</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>23003</th>\n","      <td>46006</td>\n","      <td>.. tr∆∞·ªõc khi b·ªã b·∫Øt, b√† nguy·ªÖn ph∆∞∆°ng h·∫±ng ƒë√£ ...</td>\n","      <td>0</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>23004 rows √ó 4 columns</p>\n","</div>"],"text/plain":["          id                                               text  label  \\\n","0          1  Ch·∫Øc c≈©ng bi·∫øt ko t·ªìn t·∫°i ƒëc bao l√¢u n·ªØa n√™n c...      1   \n","1          9  Emma Nguyen v·∫≠y m√† ca ng·ª£i b√™n kia l·∫Øm... V·ªÅ l...      1   \n","2         11  Nhung Pham Ai ch√™ ngh√®o ngta c√≥ ti·ªÅn ƒëi n∆∞·ªõc n...      1   \n","3         13                        ·ª¶a b√™ ƒë√™ c√≥ lol h·∫£ ta ??üòÇüòÇüòÇ      1   \n","4         16             L√†m th√™m ch·ª•c c·ªß m·ªü h√≤m csgo anh √™i :V      0   \n","...      ...                                                ...    ...   \n","22999  46002   trong tr∆∞·ªùng h·ª£p n√†y, theo ls. khanh, b√† h·∫±ng...      1   \n","23000  46003   b√† c≈©ng kh√¥ng √≠t l·∫ßn ch·ªâ tr√≠ch n·∫∑ng n·ªÅ nh·ªØng ...      0   \n","23001  46004   ‚Äúb√† nguy·ªÖn ph∆∞∆°ng h·∫±ng c√≥ nhi·ªÅu bi·ªÉu hi·ªán ƒë√£ ...      0   \n","23002  46005   v≈© ƒë·ª©c khanh n√≥i. b√† nguy·ªÖn ph∆∞∆°ng h·∫±ng b·∫Øt ƒë...      0   \n","23003  46006  .. tr∆∞·ªõc khi b·ªã b·∫Øt, b√† nguy·ªÖn ph∆∞∆°ng h·∫±ng ƒë√£ ...      0   \n","\n","      dataset_type  \n","0            train  \n","1            train  \n","2            train  \n","3            train  \n","4            train  \n","...            ...  \n","22999         test  \n","23000         test  \n","23001         test  \n","23002         test  \n","23003         test  \n","\n","[23004 rows x 4 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","0    19039\n","1     3965\n","Name: count, dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_data.label.value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def up_sample(df: pd.DataFrame):\n","    df_0 = df[df.label==0]\n","    df_1 = df[df.label==1]\n","    df_1_upsampled = df_1.sample(n=len(df_0), replace=True)\n","    return pd.concat([df_0, df_1_upsampled]).sample(frac=1)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def split_df(df: pd.DataFrame):\n","    '''\n","    test size: 0 -> 1\n","    '''\n","    \n","    return train_test_split(df, test_size= args['test_size'], random_state=42)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def process_train(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.dropna()\n","    df = df.drop_duplicates()\n","    df = df.reset_index(drop=True)\n","    \n","    train_df, val_df = split_df(df)\n","    train_df = up_sample(train_df)\n","    train_df.reset_index(drop=True, inplace=True)\n","    val_df.reset_index(drop=True, inplace=True)\n","    \n","    \n","    return train_df, val_df"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["train_data, val_data= process_train(train_data)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>dataset_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>28996</td>\n","      <td>∆† ... ! SaÃÅng naÃÄo t√¥i cuÃÉng u√¥ÃÅng cafe ƒëen t∆∞...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33230</td>\n","      <td>C√¥n ƒë·ªì, l·∫•y nh·∫ßm ch·ªìng ko ch·ªâ h·∫°i m√¨nh m√† li√™n...</td>\n","      <td>1</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38433</td>\n","      <td>ƒë√≥ kh√¥ng ph·∫£i l√† m·ªôt d√¢n t·ªôc ‚Äúanh h√πng‚Äù, m√† l...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>34380</td>\n","      <td>T·ª´ nay h√πng hung hƒÉng h√πng h·ªï b·ªõt l·∫°i nha, ch·ª©...</td>\n","      <td>1</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4603</td>\n","      <td>Ch·ªâ s·ª£ mu·ªôn qu√° ch·ªã ∆°i em ƒëang thi·∫øu n·ª£ 10 tri...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>30379</th>\n","      <td>6469</td>\n","      <td>B·ªçn b√≤ th√¨ v·∫´n b·∫£o nhau r·∫±ng: t·ª± h√†o v√¨ √¥ng ch...</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>30380</th>\n","      <td>3292</td>\n","      <td>V√†o ƒë√≥ t·ª´ h oke ch∆∞a ??</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>30381</th>\n","      <td>25261</td>\n","      <td>Th√¨ GATO v·ªõi Ho√†i Linh v√† c√°c ngh·ªá sƒ© h·∫£i ngo·∫°...</td>\n","      <td>0</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>30382</th>\n","      <td>33701</td>\n","      <td>Gojek m·ªõi huy ƒë·ªông ƒë∆∞·ª£c 1 t·ªâ usd, b√¢y h t·ªõi l∆∞...</td>\n","      <td>0</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>30383</th>\n","      <td>45293</td>\n","      <td>quy·∫øt ƒë·ªãnh tƒÉng gi√° xƒÉng d·∫ßu c·ªßa ban l√£nh ƒë·∫°o ...</td>\n","      <td>1</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30384 rows √ó 4 columns</p>\n","</div>"],"text/plain":["          id                                               text  label  \\\n","0      28996  ∆† ... ! SaÃÅng naÃÄo t√¥i cuÃÉng u√¥ÃÅng cafe ƒëen t∆∞...      0   \n","1      33230  C√¥n ƒë·ªì, l·∫•y nh·∫ßm ch·ªìng ko ch·ªâ h·∫°i m√¨nh m√† li√™n...      1   \n","2      38433   ƒë√≥ kh√¥ng ph·∫£i l√† m·ªôt d√¢n t·ªôc ‚Äúanh h√πng‚Äù, m√† l...      0   \n","3      34380  T·ª´ nay h√πng hung hƒÉng h√πng h·ªï b·ªõt l·∫°i nha, ch·ª©...      1   \n","4       4603  Ch·ªâ s·ª£ mu·ªôn qu√° ch·ªã ∆°i em ƒëang thi·∫øu n·ª£ 10 tri...      0   \n","...      ...                                                ...    ...   \n","30379   6469  B·ªçn b√≤ th√¨ v·∫´n b·∫£o nhau r·∫±ng: t·ª± h√†o v√¨ √¥ng ch...      1   \n","30380   3292                            V√†o ƒë√≥ t·ª´ h oke ch∆∞a ??      0   \n","30381  25261  Th√¨ GATO v·ªõi Ho√†i Linh v√† c√°c ngh·ªá sƒ© h·∫£i ngo·∫°...      0   \n","30382  33701  Gojek m·ªõi huy ƒë·ªông ƒë∆∞·ª£c 1 t·ªâ usd, b√¢y h t·ªõi l∆∞...      0   \n","30383  45293  quy·∫øt ƒë·ªãnh tƒÉng gi√° xƒÉng d·∫ßu c·ªßa ban l√£nh ƒë·∫°o ...      1   \n","\n","      dataset_type  \n","0            train  \n","1              dev  \n","2            train  \n","3              dev  \n","4            train  \n","...            ...  \n","30379        train  \n","30380        train  \n","30381         test  \n","30382          dev  \n","30383         test  \n","\n","[30384 rows x 4 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>30384.000000</td>\n","      <td>30384.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>22410.263099</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>14355.976338</td>\n","      <td>0.500008</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>8325.750000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>22874.500000</td>\n","      <td>0.500000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>34813.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>46006.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id         label\n","count  30384.000000  30384.000000\n","mean   22410.263099      0.500000\n","std    14355.976338      0.500008\n","min        1.000000      0.000000\n","25%     8325.750000      0.000000\n","50%    22874.500000      0.500000\n","75%    34813.000000      1.000000\n","max    46006.000000      1.000000"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["train_data.describe()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","0    15192\n","1    15192\n","Name: count, dtype: int64"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["train_data.label.value_counts()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>4601.000000</td>\n","      <td>4601.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>24363.425560</td>\n","      <td>0.163877</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>12492.293956</td>\n","      <td>0.370205</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>13.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>14105.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>24065.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>34452.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>46005.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id        label\n","count   4601.000000  4601.000000\n","mean   24363.425560     0.163877\n","std    12492.293956     0.370205\n","min       13.000000     0.000000\n","25%    14105.000000     0.000000\n","50%    24065.000000     0.000000\n","75%    34452.000000     0.000000\n","max    46005.000000     1.000000"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["val_data.describe()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","0    3847\n","1     754\n","Name: count, dtype: int64"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["val_data.label.value_counts()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>23003.000000</td>\n","      <td>23003.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>21564.629483</td>\n","      <td>0.193497</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>13701.699345</td>\n","      <td>0.395047</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>10304.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>18725.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>32791.500000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>44132.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id         label\n","count  23003.000000  23003.000000\n","mean   21564.629483      0.193497\n","std    13701.699345      0.395047\n","min        0.000000      0.000000\n","25%    10304.000000      0.000000\n","50%    18725.000000      0.000000\n","75%    32791.500000      0.000000\n","max    44132.000000      1.000000"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_data.describe()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","0    18552\n","1     4451\n","Name: count, dtype: int64"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["test_data.label.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["# **Dataset, Dataloader**"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import string"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:18.076611Z","iopub.status.busy":"2024-01-17T08:20:18.075617Z","iopub.status.idle":"2024-01-17T08:20:18.083388Z","shell.execute_reply":"2024-01-17T08:20:18.082422Z","shell.execute_reply.started":"2024-01-17T08:20:18.076562Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:5: SyntaxWarning: invalid escape sequence '\\/'\n","<>:11: SyntaxWarning: invalid escape sequence '\\w'\n","<>:5: SyntaxWarning: invalid escape sequence '\\/'\n","<>:11: SyntaxWarning: invalid escape sequence '\\w'\n","/tmp/ipykernel_2443495/3853289160.py:5: SyntaxWarning: invalid escape sequence '\\/'\n","  text = re.sub('https:\\/\\/\\S+', '', text)\n","/tmp/ipykernel_2443495/3853289160.py:11: SyntaxWarning: invalid escape sequence '\\w'\n","  text = re.sub('\\w*\\d\\w*', '', text)\n"]}],"source":["def preprocess_text(text):\n","    # to lower case\n","    text = text.lower()\n","    # remove links\n","    text = re.sub('https:\\/\\/\\S+', '', text) \n","    # remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n","    # remove next line     \n","    text = re.sub(r'[^ \\w\\.]', '', text) \n","    # remove words containing numbers\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["### **Data and Model**"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bstrai/multilingual-toxic-xlm-roberta\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"bstrai/multilingual-toxic-xlm-roberta\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 16\n","MAX_LEN = 300"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:18.747775Z","iopub.status.busy":"2024-01-17T08:20:18.747403Z","iopub.status.idle":"2024-01-17T08:20:18.758753Z","shell.execute_reply":"2024-01-17T08:20:18.757768Z","shell.execute_reply.started":"2024-01-17T08:20:18.747743Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    \n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.text.apply(preprocess_text)\n","        self.targets = torch.Tensor(list(self.data.label)).reshape(-1, 1)\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            # padding= \"max_length\",\n","            pad_to_max_length=True,\n","\n","            return_token_type_ids=True,\n","            return_tensors = 'pt',\n","        )\n","        ids = inputs['input_ids'].squeeze()\n","        mask = inputs['attention_mask'].squeeze()\n","        token_type_ids = inputs[\"token_type_ids\"].squeeze()\n","\n","        \n","        return {\n","            # 'ids': torch.tensor(ids, dtype=torch.long),\n","            # 'mask': torch.tensor(mask, dtype=torch.long),\n","            # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            # 'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","            'ids': ids.clone().detach().long(),\n","            'mask': mask.clone().detach().long(),\n","            'token_type_ids': token_type_ids.clone().detach().long(),\n","            'targets': self.targets[index].clone().detach().long(),\n","            \n","        }\n","        "]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:19.030490Z","iopub.status.busy":"2024-01-17T08:20:19.030193Z","iopub.status.idle":"2024-01-17T08:20:20.564500Z","shell.execute_reply":"2024-01-17T08:20:20.563414Z","shell.execute_reply.started":"2024-01-17T08:20:19.030465Z"},"trusted":true},"outputs":[],"source":["MAX_LEN = args['max_length']\n","BATCH_SIZE = args['batch_size']\n","train_dataset = CustomDataset(dataframe= train_data, tokenizer= tokenizer, max_len= MAX_LEN)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:20.566397Z","iopub.status.busy":"2024-01-17T08:20:20.566119Z","iopub.status.idle":"2024-01-17T08:20:20.583892Z","shell.execute_reply":"2024-01-17T08:20:20.582983Z","shell.execute_reply.started":"2024-01-17T08:20:20.566374Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["{'ids': tensor([     0,      6,  12435,   6079,     24,     31,   2259,  16180,     75,\n","         23366,  43185,  58095,   4797,   3713,   1337, 150421,   2933,   7630,\n","         75515,     14,      2,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0])}\n"]},{"name":"stderr","output_type":"stream","text":["/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["for sample in train_dataset:\n","    print(sample)\n","    break"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["val_dataset = CustomDataset(dataframe= val_data, tokenizer= tokenizer, max_len= MAX_LEN)\n","test_dataset = CustomDataset(dataframe= test_data, tokenizer= tokenizer, max_len= MAX_LEN)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True, num_workers= 4)\n","val_loader = DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle= False, num_workers= 4)\n","test_loader = DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle= False, num_workers= 4)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["8"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["train_loader.batch_size"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 300])\n"]}],"source":["input_ids = next(iter(train_loader))['ids']\n","print(input_ids.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Model**"]},{"cell_type":"markdown","metadata":{},"source":["### **Bert**"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["XLMRobertaClassificationHead(\n","  (dense): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (out_proj): Linear(in_features=768, out_features=16, bias=True)\n",")"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["in_features = model.classifier\n","in_features"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["XLMRobertaForSequenceClassification(\n","  (roberta): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): XLMRobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n","  )\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","metadata":{},"source":["# **Model**"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-01-17T08:20:37.067971Z","iopub.status.busy":"2024-01-17T08:20:37.067657Z","iopub.status.idle":"2024-01-17T08:20:37.086444Z","shell.execute_reply":"2024-01-17T08:20:37.085558Z","shell.execute_reply.started":"2024-01-17T08:20:37.067942Z"},"trusted":true},"outputs":[],"source":["# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n","\n","class Toxic_RoBERTa(torch.nn.Module):\n","    def __init__(self, mode):\n","        super(Toxic_RoBERTa, self).__init__()\n","        self.back_bone = AutoModelForSequenceClassification.from_pretrained(\"bstrai/multilingual-toxic-xlm-roberta\")\n","        if mode == 'transfer':\n","            for param in self.back_bone.parameters():\n","                param.requires_grad = False\n","        else:\n","            for param in self.back_bone.parameters():\n","                param.requires_grad = True\n","        self.fc = torch.nn.Linear(16, 1)\n","\n","    def forward(self, ids, mask, token_type_ids):\n","        output_1 = self.back_bone(ids, attention_mask = mask, token_type_ids= token_type_ids, return_dict=False)[0]\n","        output = self.fc(output_1)\n","        return output\n","    \n","    \n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["def evaluate(model, data_loader: DataLoader):\n","    model.to(device)\n","    MAX_LEN = 300\n","    BATCH_SIZE = 16\n","    \n","    model.eval()\n","    fin_targets=[]\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, data in enumerate(data_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            \n","            targets = data['targets'].to(device, dtype = torch.float)\n","            \n","            outputs = model(ids, mask, token_type_ids)\n","            fin_targets.extend(targets.clone().cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).clone().cpu().detach().numpy().tolist())\n","#                 \n","            \n","    outputs = np.array(fin_outputs) >= 0.5\n","    accuracy = metrics.accuracy_score(fin_targets, outputs)\n","    f1_score = metrics.f1_score(fin_targets, outputs, average='binary')\n","    print(f\"Accuracy Score = {accuracy}\")\n","    print(f\"F1 Score = {f1_score}\")\n","    \n","    return fin_outputs, fin_targets, accuracy, f1_score\n","\n","def predict(model, text):\n","    model.to(device)\n","    model.eval()\n","    inputs = tokenizer.encode_plus(text, \n","                        add_special_tokens=True, \n","                        return_token_type_ids= True,\n","                        max_length=MAX_LEN, \n","                        padding=\"max_length\", \n","                        return_tensors='pt') \n","    \n","    ids = inputs['input_ids'].to(device)\n","    mask = inputs['attention_mask'].to(device)\n","    token_type_ids = inputs[\"token_type_ids\"].to(device)\n","    # print(ids.shape)\n","\n","    # ids = ids.to(device)\n","    # mask = mask.to(device)\n","    # token_type_ids = token_type_ids.to(device)\n","            \n","    outputs = model(ids, mask, token_type_ids)\n","    return torch.sigmoid(outputs).clone().cpu().detach().numpy().tolist()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def get_model(mode= 'fine_tune', weight_file= None):\n","    \n","    model = Toxic_RoBERTa(mode= mode)\n","    # print('a')\n","    \n","    if weight_file:\n","        print(\"load weight from\", weight_file)\n","        model.load_state_dict(torch.load(weight_file, map_location= device))\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# **Train**"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def get_Loss_func():\n","    return torch.nn.BCEWithLogitsLoss()\n","\n","def model_step(batch, model, loss_func, device):\n","    ids = batch['ids'].to(device, dtype = torch.long)\n","    mask = batch['mask'].to(device, dtype = torch.long)\n","    token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n","    targets = batch['targets'].to(device, dtype = torch.float)\n","\n","    outputs = model(ids, mask, token_type_ids)\n","    loss = loss_func(outputs, targets)\n","    preds = outputs >= 0.5\n","\n","    return loss, preds, targets\n","\n","def optimizer_step(optimizer, loss):\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","def evaluation(model, loader, loss_func, epoch, device, ):\n","    model.eval()\n","    total, total_loss, total_correct = 0, 0, 0\n","    preds_labels = []\n","    y_labels = []\n","    pbar = tqdm(loader)\n","    cnt = 0\n","    for batch in pbar:\n","        loss, preds, y = model_step(batch, model, loss_func, device)\n","\n","        total += len(y)\n","        total_loss += loss.item()\n","        total_correct += torch.sum(preds == y).item()\n","        pbar.set_description(f\"epoch = {epoch}, val/loss = {total_loss/total:.3f}, val/acc = {total_correct/total:.3f}\")\n","        preds_labels.extend(preds.clone().cpu().detach().numpy().tolist())\n","        y_labels.extend(y.clone().cpu().detach().numpy().tolist())\n","    \n","    accuracy = accuracy_score(y_labels, preds_labels)\n","    precision = precision_score(y_labels, preds_labels)\n","    recall = recall_score(y_labels, preds_labels)\n","    f1 = f1_score(y_labels, preds_labels)\n","    \n","    wandb.log({\"val_loss\": total_loss/total, \"val_acc\": total_correct/total, \"val_precision\": precision, \"val_recall\": recall, \"val_f1\": f1})\n","    \n","\n","    return (total_loss / total), (total_correct/total)    #loss, accuracy\n","\n","def train_epoch(model, train_loader, loss_func, optimizer, epoch, device):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    total, total_loss, total_correct = 0, 0, 0\n","    cnt = 0\n","    for batch in pbar:\n","        loss, preds, y = model_step(batch, model, loss_func, device)\n","        optimizer_step(optimizer, loss)\n","\n","        total += len(y)\n","        total_loss += loss.item()\n","        total_correct += torch.sum(preds == y).item()\n","        pbar.set_description(f\"epoch = {epoch}, train/loss = {total_loss/total:.3f}, train/acc = {total_correct/total:.3f} \")\n","        \n","        if cnt % 40 == 0:\n","            wandb.log({\"train_loss\": total_loss/total, \"train_acc\": total_correct/total})\n","        cnt += 1\n","    \n","    return (total_loss / total), (total_correct/total)    #loss, accuracy\n","\n","\n","def plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history):\n","    plt.figure(figsize=(8, 8))\n","    plt.subplot(2, 1, 1)\n","    plt.plot(train_acc_history, label='Training Accuracy')\n","    plt.plot(val_acc_history, label='Validation Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.ylabel('Accuracy')\n","    plt.title('Training and Validation Accuracy')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.plot(train_loss_history, label='Training Loss')\n","    plt.plot(val_loss_history, label='Validation Loss')\n","    plt.legend(loc='upper right')\n","    plt.ylabel('BCElogits Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('epoch')\n","    plt.show()\n","\n","def train(model, train_loader, val_loader, lr=1e-3, max_epochs=1, device = \"cuda\", plot_res = True, output_file=None):\n","    train_loss_history = []\n","    train_acc_history = []\n","    val_loss_history = []\n","    val_acc_history = []\n","    loss_func = get_Loss_func()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    best_score = 0\n","    for epoch in range(max_epochs):\n","        ## train loop\n","        train_loss, train_acc = train_epoch(model, train_loader, loss_func, optimizer, epoch, device)\n","        train_loss_history.append(train_loss)\n","        train_acc_history.append(train_acc)\n","        ## val loop\n","        val_loss, val_acc = evaluation(model, val_loader, loss_func, epoch, device)\n","        val_loss_history.append(val_loss)\n","        val_acc_history.append(val_acc)\n","        # wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n","        \n","        if output_file and val_acc > best_score:\n","            best_score = val_acc\n","            save_weight(model, output_file)\n","            \n","    if(plot_res):\n","        plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n","    return (train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n","\n","def save_weight(model, output_file):\n","    print(\"save model to\", output_file)\n","    torch.save(model.state_dict(), output_file[:-4] + \"_best\" + \".pth\")\n","    \n","def main(mode=\"fine-tune\", weight_file=None, output_file=None, epochs= None, device= 'cuda'):\n","\n","    if mode == \"fine-tune\" and weight_file:\n","        model = get_model(mode=mode, weight_file=weight_file)\n","    else:\n","        model = get_model(mode=mode)\n","    \n","    model.to(device)\n","    wandb.watch(model, log=\"all\")\n","    lr = 1e-3 if mode == \"transfer\" else 1e-5\n","    max_epochs = epochs \n","    train(model, train_loader, val_loader, device=device, max_epochs=max_epochs, lr=lr, output_file=output_file)\n","\n","    if output_file:\n","        # \n","        save_weight(model, output_file)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["output_fine_tune_link = 'ckpt/model-fine-tune1.2.pth'\n","output_transfer_link = 'ckpt/model-transfer.pth'"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["load weight from ckpt/model-fine-tune1.2_best.pth\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# output_folder_link = 'ckpt'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# main(mode=\"transfer\", output_file= output_transfer_link, device= device, epochs= 1)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine-tune\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mckpt/model-fine-tune1.2_best.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_fine_tune_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine_tune_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[42], line 113\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode, weight_file, output_file, epochs, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tune\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tune\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m weight_file:\n\u001b[0;32m--> 113\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         model \u001b[38;5;241m=\u001b[39m get_model(mode\u001b[38;5;241m=\u001b[39mmode)\n","Cell \u001b[0;32mIn[41], line 8\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(mode, weight_file)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_file:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload weight from\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_file)\n\u001b[0;32m----> 8\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n","File \u001b[0;32m/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/torch/serialization.py:1373\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# output_folder_link = 'ckpt'\n","\n","# main(mode=\"transfer\", output_file= output_transfer_link, device= device, epochs= 1)\n","main(mode=\"fine-tune\", weight_file= \"ckpt/model-fine-tune1.2_best.pth\", output_file= output_fine_tune_link, device= device, epochs= args['fine_tune_epochs'])\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Validating the Model\n","\n","As defined above to get a measure of our models performance we are using the following metrics.\n","- Accuracy Score\n","- F1 Micro\n","- F1 Macro"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["load weight from ckpt/model-fine-tune1.2_best.pth\n"]},{"data":{"text/plain":["Toxic_RoBERTa(\n","  (back_bone): XLMRobertaForSequenceClassification(\n","    (roberta): XLMRobertaModel(\n","      (embeddings): XLMRobertaEmbeddings(\n","        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): XLMRobertaEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x XLMRobertaLayer(\n","            (attention): XLMRobertaAttention(\n","              (self): XLMRobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): XLMRobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): XLMRobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): XLMRobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (classifier): XLMRobertaClassificationHead(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (out_proj): Linear(in_features=768, out_features=16, bias=True)\n","    )\n","  )\n","  (fc): Linear(in_features=16, out_features=1, bias=True)\n",")"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# model = get_model(weight_file= output_fine_tune_link)\n","model = get_model(weight_file= \"ckpt/model-fine-tune1.2_best.pth\")\n","\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/work/hpc/miniconda3/envs/hieu-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy Score = 0.8471068991001174\n","F1 Score = 0.5638099962793005\n"]}],"source":["final_output, final_target, acc, f1 = evaluate(model, test_loader)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0],\n","       [0],\n","       [0],\n","       ...,\n","       [1],\n","       [1],\n","       [0]])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["output = (np.array(final_output) >= 0.5).astype(int)\n","output"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0],\n","       [0],\n","       [0],\n","       ...,\n","       [1],\n","       [1],\n","       [0]])"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["final_target = np.array(final_target).astype(int)\n","final_target"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["del recall_score, f1_score, precision_score"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy:  0.8471068991001174\n","recall:  0.510671759155246\n","precision:  0.6292912513842747\n","f1:  0.5638099962793005\n"]}],"source":["accuracy = accuracy_score(final_target, output)\n","recall = recall_score(final_target, output)\n","precision = precision_score(final_target, output)\n","f1 = f1_score(final_target, output)\n","print(f'accuracy: ', accuracy)\n","print(f'recall: ', recall)\n","print(f'precision: ', precision)\n","print(f'f1: ', f1)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["test_data['predict_label'] = output\n","test_data['predict_label'] = test_data['predict_label'].astype(int)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["mislabel = test_data[test_data.label != test_data.predict_label]"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>dataset_type</th>\n","      <th>predict_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7</th>\n","      <td>32375</td>\n","      <td>T√¥i kh√¥ng hi·ªÉu sao l·∫°i ƒë·ªÉ v·∫≠y.</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>5553</td>\n","      <td>Kh√¥ng bi·∫øt t·ªët x·∫•u! Ch·∫•t ph√°t nh∆∞ n√†o nh∆∞ng ng...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>7102</td>\n","      <td>con naÃÄy th∆∞Ã£c s∆∞Ã£ noÃÅ gi√¥ÃÅng nh∆∞ con thuÃÅ nh√¥...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>43278</td>\n","      <td>chuy·ªán ƒë·ªùi c·ªông s·∫£n l∆∞u vong (7) ***** 21) m∆∞·ªù...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>6289</td>\n","      <td>C√°i qu·∫ßn in h·ªát c√°i √Ω th·ª©c</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22965</th>\n","      <td>42839</td>\n","      <td>B·ªò C√îNG AN TUY·ªÇN H∆†N 16.000 L√çNH CSCƒê NƒÉm 2024...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22980</th>\n","      <td>40625</td>\n","      <td>v·∫≠y ƒëi·ªÅu 331 b·ªô lu·∫≠t h√¨nh s·ª± 2015 c√≥ n·ªôi dung...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22981</th>\n","      <td>37954</td>\n","      <td>khu k√Ω t√∫c x√° n·∫±m g·∫ßn v·ªõi b·ªánh vi·ªán ƒëa khoa t...</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22983</th>\n","      <td>3196</td>\n","      <td>T√∫ Trinh ko ch·∫°y ƒÉn c·ª©t, ƒÉn ƒëb √† üôÑ</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22994</th>\n","      <td>38935</td>\n","      <td>42.000 ‚Äúb·ªã h·∫°‚Äù c·ªßa ch∆∞∆°ng tr√¨nh vua tin v·ªãt! m...</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3517 rows √ó 5 columns</p>\n","</div>"],"text/plain":["          id                                               text  label  \\\n","7      32375                     T√¥i kh√¥ng hi·ªÉu sao l·∫°i ƒë·ªÉ v·∫≠y.      0   \n","22      5553  Kh√¥ng bi·∫øt t·ªët x·∫•u! Ch·∫•t ph√°t nh∆∞ n√†o nh∆∞ng ng...      1   \n","29      7102  con naÃÄy th∆∞Ã£c s∆∞Ã£ noÃÅ gi√¥ÃÅng nh∆∞ con thuÃÅ nh√¥...      1   \n","36     43278  chuy·ªán ƒë·ªùi c·ªông s·∫£n l∆∞u vong (7) ***** 21) m∆∞·ªù...      1   \n","48      6289                         C√°i qu·∫ßn in h·ªát c√°i √Ω th·ª©c      1   \n","...      ...                                                ...    ...   \n","22965  42839  B·ªò C√îNG AN TUY·ªÇN H∆†N 16.000 L√çNH CSCƒê NƒÉm 2024...      1   \n","22980  40625   v·∫≠y ƒëi·ªÅu 331 b·ªô lu·∫≠t h√¨nh s·ª± 2015 c√≥ n·ªôi dung...      1   \n","22981  37954   khu k√Ω t√∫c x√° n·∫±m g·∫ßn v·ªõi b·ªánh vi·ªán ƒëa khoa t...      0   \n","22983   3196                 T√∫ Trinh ko ch·∫°y ƒÉn c·ª©t, ƒÉn ƒëb √† üôÑ      1   \n","22994  38935  42.000 ‚Äúb·ªã h·∫°‚Äù c·ªßa ch∆∞∆°ng tr√¨nh vua tin v·ªãt! m...      1   \n","\n","      dataset_type  predict_label  \n","7            train              1  \n","22           train              0  \n","29           train              0  \n","36           train              0  \n","48           train              0  \n","...            ...            ...  \n","22965        train              0  \n","22980        train              0  \n","22981        train              1  \n","22983        train              0  \n","22994        train              0  \n","\n","[3517 rows x 5 columns]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["mislabel"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["mislabel.to_csv('Iterations/Iter1/1.2/mislabel.csv', index= False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6834277,"sourceId":62805,"sourceType":"competition"},{"datasetId":4308755,"sourceId":7408364,"sourceType":"datasetVersion"},{"datasetId":4311471,"sourceId":7412241,"sourceType":"datasetVersion"},{"datasetId":4315958,"sourceId":7418746,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
